{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78DVNsTd_6ns"
   },
   "source": [
    "Note: Each section can be run independently of others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh5IIJ2P_qRf"
   },
   "source": [
    "# Imports, Installs, Downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "YxXIRm6fwZiJ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import ast\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Im1DI5BewZiO",
    "outputId": "ca5606eb-6779-4b2c-abd3-56024ed9eea7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/aditiroy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/aditiroy/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/aditiroy/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFFkQqz2wZiM"
   },
   "source": [
    "# Emotion Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "aKESyNPKwZiP"
   },
   "outputs": [],
   "source": [
    "def load_nrc_lexicon():\n",
    "    emotion_lexicon = defaultdict(list)\n",
    "    lexicon_file = \"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "\n",
    "    with open(lexicon_file, 'r') as file:\n",
    "        for line in file:\n",
    "            word, emotion, association = line.strip().split('\\t')\n",
    "            if int(association) == 1:\n",
    "                emotion_lexicon[word].append(emotion)\n",
    "    return emotion_lexicon\n",
    "\n",
    "def preprocess_lyrics(lyrics):\n",
    "    punctuations = '\\'\"\\\\,<>./?@#$%^&*_~/!()-[]{};:'\n",
    "    # Remove punctuation and any content within brackets (e.g., [chorus])\n",
    "    lyrics = ''.join([char for char in lyrics if char not in punctuations])\n",
    "    lyrics = lyrics.split('[')[0]  # Remove anything between [ and ]\n",
    "\n",
    "    tokens = nltk.word_tokenize(lyrics.lower())\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def normalize_distribution(distribution):\n",
    "    total = sum(distribution.values())\n",
    "    if total > 0:\n",
    "        return {emotion: value / total for emotion, value in distribution.items()}\n",
    "    else:\n",
    "        return {emotion: 0 for emotion in distribution.keys()}  # Return zero for all if total is zero\n",
    "\n",
    "def assign_emotion_to_song(lyrics, emotion_lexicon, tfidf_vector, tfidf_words):\n",
    "    emotion_count = defaultdict(int)\n",
    "    sentiment_count = defaultdict(int)\n",
    "    tokens = preprocess_lyrics(lyrics)\n",
    "\n",
    "    # For each token, check if it's in the emotion lexicon\n",
    "    for word in tokens:\n",
    "        if word in emotion_lexicon:\n",
    "            # Use np.where to get the index of the word in tfidf_words\n",
    "            word_index_array = np.where(tfidf_words == word)[0]\n",
    "            if len(word_index_array) > 0:\n",
    "                word_index = word_index_array[0]\n",
    "                word_tfidf_score = tfidf_vector[word_index]\n",
    "                for emotion in emotion_lexicon[word]:\n",
    "                    if emotion in ['positive', 'negative']:\n",
    "                        sentiment_count[emotion] += word_tfidf_score\n",
    "                    else:\n",
    "                        emotion_count[emotion] += word_tfidf_score\n",
    "\n",
    "    # Normalize distributions\n",
    "    normalized_emotion_count = normalize_distribution(emotion_count)\n",
    "    normalized_sentiment_count = normalize_distribution(sentiment_count)\n",
    "\n",
    "    # Determine dominant emotion and dominant sentiment\n",
    "    dominant_emotion = max(normalized_emotion_count, key=normalized_emotion_count.get) if normalized_emotion_count else None\n",
    "    dominant_sentiment = max(normalized_sentiment_count, key=normalized_sentiment_count.get) if normalized_sentiment_count else None\n",
    "\n",
    "    return dominant_emotion, dominant_sentiment, normalized_emotion_count, normalized_sentiment_count\n",
    "\n",
    "def assign_emotions_to_dataset(df, emotion_lexicon, tfidf_scores, tfidf_words):\n",
    "    emotion_results = []\n",
    "\n",
    "    # Loop over the songs in the dataset\n",
    "    for index, row in df.iterrows():\n",
    "        song_id = row['id']\n",
    "        artist = row['artist']\n",
    "        title = row['title']\n",
    "        lyrics = row['lyrics']\n",
    "\n",
    "        # Get the corresponding TF-IDF vector for the song\n",
    "        if index < len(tfidf_scores):\n",
    "            tfidf_vector = tfidf_scores[index]\n",
    "        else:\n",
    "            continue  # Skip if the index is out of bounds\n",
    "\n",
    "        dominant_emotion, dominant_sentiment, emotion_count, sentiment_count = assign_emotion_to_song(lyrics, emotion_lexicon, tfidf_vector, tfidf_words)\n",
    "\n",
    "        emotion_results.append({\n",
    "            'song_id': song_id,\n",
    "            'artist': artist,\n",
    "            'title': title,\n",
    "            'dominant_emotion': dominant_emotion,\n",
    "            'dominant_sentiment': dominant_sentiment,\n",
    "            'emotion_distribution': emotion_count,\n",
    "            'sentiment_distribution': sentiment_count\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(emotion_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "cal-pdfYwZiQ",
    "outputId": "af36fc78-ff68-4035-ece8-827a5ebba8cd"
   },
   "outputs": [],
   "source": [
    "# df = pd.read_csv('song_lyrics.csv')\n",
    "# df = df[df['language'] == 'en']\n",
    "# df = df.dropna()\n",
    "# sample_df = df.sample(n = 1000)\n",
    "# sample_df.to_csv('sample_df.csv', index=False)\n",
    "\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('song_lyrics.csv')\n",
    "df_songs = df[df['language'] == 'en']\n",
    "df_songs = df_songs[(df_songs['year'] >= 1950) & (df_songs['year'] <= 2025) & (df_songs['views'] >= 100)]\n",
    "df_songs = df_songs.dropna()\n",
    "\n",
    "# Load NRC emotion lexicon\n",
    "emotion_lexicon = load_nrc_lexicon()\n",
    "\n",
    "# Preprocess the lyrics and create a list of all lyrics\n",
    "lyrics_list = df_songs['lyrics'].apply(lambda x: ' '.join(preprocess_lyrics(x)))\n",
    "\n",
    "# Compute TF-IDF for the lyrics\n",
    "vectorizer = TfidfVectorizer(max_features=10000)  # Limit to top 10,000 words\n",
    "tfidf_scores = vectorizer.fit_transform(lyrics_list).toarray()\n",
    "tfidf_words = vectorizer.get_feature_names_out()  # List of words\n",
    "\n",
    "# Assign emotions to all songs\n",
    "emotion_results_df = assign_emotions_to_dataset(df_songs, emotion_lexicon, tfidf_scores, tfidf_words)\n",
    "emotion_results_df = emotion_results_df.dropna()\n",
    "\n",
    "# Save the results to a CSV file\n",
    "emotion_results_df.to_csv('emotion_assigned_songs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hSZibbJwZiQ"
   },
   "source": [
    "# Emotion Classification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "vz9zwEMlwZiR"
   },
   "outputs": [],
   "source": [
    "# get all survey data and format\n",
    "survey_data = {\n",
    "    'song_id' : [6636486] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.15, 0.10, 0.20, 0.05, 0.10, 0.05, 0.20, 0.15, 0.05, 0.10],\n",
    "    'Joy': [0.10, 0.05, 0.05, 0.05, 0.15, 0.10, 0.05, 0.05, 0.10, 0.10],\n",
    "    'Surprise': [0.05, 0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05, 0.10, 0.10],\n",
    "    'Anger': [0.30, 0.40, 0.25, 0.35, 0.25, 0.30, 0.25, 0.35, 0.30, 0.30],\n",
    "    'Disgust': [0.10, 0.05, 0.10, 0.15, 0.05, 0.10, 0.10, 0.05, 0.05, 0.05],\n",
    "    'Fear': [0.15, 0.10, 0.15, 0.10, 0.20, 0.15, 0.10, 0.10, 0.10, 0.10],\n",
    "    'Sadness': [0.10, 0.20, 0.10, 0.10, 0.10, 0.15, 0.15, 0.15, 0.20, 0.15],\n",
    "    'Trust': [0.05, 0.05, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10, 0.10]\n",
    "}\n",
    "survey_1 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [5955393] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.10, 0.10, 0.15, 0.05, 0.10, 0.05, 0.20, 0.05, 0.10, 0.15],\n",
    "    'Joy': [0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05, 0.10, 0.05, 0.05],\n",
    "    'Surprise': [0.05, 0.05, 0.10, 0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Anger': [0.25, 0.25, 0.30, 0.35, 0.30, 0.35, 0.25, 0.30, 0.30, 0.30],\n",
    "    'Disgust': [0.10, 0.05, 0.10, 0.15, 0.05, 0.10, 0.10, 0.05, 0.05, 0.05],\n",
    "    'Fear': [0.20, 0.15, 0.15, 0.20, 0.25, 0.20, 0.20, 0.15, 0.20, 0.25],\n",
    "    'Sadness': [0.20, 0.30, 0.25, 0.15, 0.20, 0.15, 0.25, 0.25, 0.25, 0.20],\n",
    "    'Trust': [0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
    "}\n",
    "survey_2 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [4191823] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.25, 0.30, 0.35, 0.30, 0.25, 0.20, 0.30, 0.35, 0.30, 0.25],\n",
    "    'Joy': [0.15, 0.10, 0.15, 0.10, 0.10, 0.15, 0.10, 0.10, 0.15, 0.10],\n",
    "    'Surprise': [0.20, 0.15, 0.20, 0.20, 0.15, 0.20, 0.25, 0.20, 0.15, 0.20],\n",
    "    'Anger': [0.30, 0.35, 0.40, 0.35, 0.40, 0.35, 0.30, 0.40, 0.35, 0.30],\n",
    "    'Disgust': [0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Fear': [0.15, 0.20, 0.15, 0.20, 0.15, 0.15, 0.20, 0.15, 0.20, 0.15],\n",
    "    'Sadness': [0.05, 0.10, 0.05, 0.05, 0.10, 0.05, 0.05, 0.10, 0.05, 0.05],\n",
    "    'Trust': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05]\n",
    "}\n",
    "survey_3 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [1062758] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.45, 0.40, 0.50, 0.55, 0.50, 0.45, 0.40, 0.50, 0.45, 0.50],\n",
    "    'Joy': [0.35, 0.40, 0.35, 0.30, 0.35, 0.40, 0.40, 0.35, 0.35, 0.30],\n",
    "    'Surprise': [0.05, 0.10, 0.05, 0.05, 0.05, 0.10, 0.05, 0.05, 0.10, 0.05],\n",
    "    'Anger': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Disgust': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Fear': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Sadness': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Trust': [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25]\n",
    "}\n",
    "survey_4 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [7402191] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.55, 0.60, 0.50, 0.45, 0.50, 0.55, 0.60, 0.50, 0.45, 0.50],\n",
    "    'Joy': [0.25, 0.20, 0.25, 0.30, 0.25, 0.20, 0.20, 0.25, 0.30, 0.25],\n",
    "    'Surprise': [0.15, 0.10, 0.15, 0.15, 0.10, 0.15, 0.10, 0.15, 0.15, 0.10],\n",
    "    'Anger': [0.60, 0.55, 0.60, 0.65, 0.60, 0.55, 0.55, 0.60, 0.65, 0.60],\n",
    "    'Disgust': [0.50, 0.45, 0.50, 0.55, 0.50, 0.45, 0.45, 0.50, 0.55, 0.50],\n",
    "    'Fear': [0.35, 0.30, 0.35, 0.40, 0.35, 0.30, 0.30, 0.35, 0.40, 0.35],\n",
    "    'Sadness': [0.30, 0.25, 0.30, 0.35, 0.30, 0.25, 0.25, 0.30, 0.35, 0.30],\n",
    "    'Trust': [0.20, 0.15, 0.20, 0.25, 0.20, 0.15, 0.15, 0.20, 0.25, 0.20]\n",
    "}\n",
    "survey_5 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [6047243] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.25, 0.20, 0.15, 0.25, 0.20, 0.30, 0.20, 0.25, 0.30, 0.20],\n",
    "    'Joy': [0.10, 0.05, 0.15, 0.10, 0.05, 0.10, 0.05, 0.10, 0.05, 0.05],\n",
    "    'Surprise': [0.05, 0.10, 0.05, 0.05, 0.10, 0.05, 0.05, 0.10, 0.05, 0.05],\n",
    "    'Anger': [0.50, 0.55, 0.50, 0.55, 0.60, 0.55, 0.50, 0.55, 0.60, 0.55],\n",
    "    'Disgust': [0.35, 0.40, 0.35, 0.40, 0.45, 0.40, 0.35, 0.40, 0.45, 0.40],\n",
    "    'Fear': [0.60, 0.55, 0.60, 0.65, 0.60, 0.65, 0.60, 0.65, 0.60, 0.65],\n",
    "    'Sadness': [0.70, 0.75, 0.70, 0.75, 0.80, 0.75, 0.70, 0.75, 0.80, 0.75],\n",
    "    'Trust': [0.15, 0.10, 0.15, 0.10, 0.05, 0.10, 0.15, 0.10, 0.05, 0.10]\n",
    "}\n",
    "survey_6 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [4313071] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.25, 0.20, 0.30, 0.20, 0.25, 0.30, 0.25, 0.20, 0.15, 0.25],\n",
    "    'Joy': [0.20, 0.30, 0.25, 0.30, 0.25, 0.20, 0.15, 0.20, 0.30, 0.25],\n",
    "    'Surprise': [0.10, 0.10, 0.05, 0.10, 0.10, 0.10, 0.05, 0.10, 0.10, 0.05],\n",
    "    'Anger': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.05, 0.05, 0.05],\n",
    "    'Disgust': [0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Fear': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.05, 0.05, 0.05],\n",
    "    'Sadness': [0.20, 0.20, 0.20, 0.15, 0.20, 0.15, 0.15, 0.20, 0.15, 0.15],\n",
    "    'Trust': [0.10, 0.05, 0.05, 0.05, 0.10, 0.05, 0.10, 0.05, 0.10, 0.10]\n",
    "}\n",
    "survey_7 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [6850734] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.20, 0.25, 0.20, 0.30, 0.25, 0.20, 0.15, 0.25, 0.20, 0.20],\n",
    "    'Joy': [0.15, 0.10, 0.15, 0.20, 0.10, 0.15, 0.20, 0.15, 0.10, 0.15],\n",
    "    'Surprise': [0.10, 0.15, 0.10, 0.05, 0.15, 0.10, 0.10, 0.10, 0.15, 0.10],\n",
    "    'Anger': [0.05, 0.05, 0.05, 0.10, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Disgust': [0.05, 0.10, 0.05, 0.05, 0.05, 0.05, 0.10, 0.05, 0.10, 0.05],\n",
    "    'Fear': [0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.10],\n",
    "    'Sadness': [0.20, 0.15, 0.20, 0.15, 0.20, 0.15, 0.20, 0.20, 0.20, 0.15],\n",
    "    'Trust': [0.20, 0.15, 0.15, 0.10, 0.15, 0.15, 0.15, 0.15, 0.15, 0.20]\n",
    "}\n",
    "survey_8 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [6736901] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Joy': [0.01, 0.00, 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.01, 0.00],\n",
    "    'Surprise': [0.05, 0.10, 0.05, 0.00, 0.00, 0.05, 0.05, 0.10, 0.05, 0.05],\n",
    "    'Anger': [0.10, 0.15, 0.20, 0.15, 0.10, 0.10, 0.15, 0.15, 0.10, 0.15],\n",
    "    'Disgust': [0.20, 0.25, 0.20, 0.30, 0.20, 0.25, 0.20, 0.20, 0.25, 0.20],\n",
    "    'Fear': [0.25, 0.20, 0.25, 0.25, 0.30, 0.30, 0.25, 0.20, 0.25, 0.25],\n",
    "    'Sadness': [0.30, 0.25, 0.25, 0.25, 0.30, 0.25, 0.30, 0.25, 0.25, 0.25],\n",
    "    'Trust': [0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.05, 0.00, 0.00]\n",
    "}\n",
    "survey_9 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_data = {\n",
    "    'song_id': [6184434] * 10,\n",
    "    'Participant': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    'Anticipation': [0.10, 0.15, 0.10, 0.10, 0.15, 0.10, 0.15, 0.10, 0.15, 0.10],\n",
    "    'Joy': [0.05, 0.00, 0.05, 0.00, 0.00, 0.05, 0.00, 0.05, 0.00, 0.05],\n",
    "    'Surprise': [0.10, 0.05, 0.05, 0.10, 0.10, 0.05, 0.05, 0.10, 0.05, 0.05],\n",
    "    'Anger': [0.20, 0.25, 0.30, 0.20, 0.25, 0.20, 0.25, 0.25, 0.20, 0.20],\n",
    "    'Disgust': [0.05, 0.05, 0.05, 0.05, 0.05, 0.10, 0.05, 0.05, 0.05, 0.05],\n",
    "    'Fear': [0.25, 0.20, 0.25, 0.30, 0.25, 0.25, 0.20, 0.20, 0.25, 0.25],\n",
    "    'Sadness': [0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25, 0.25],\n",
    "    'Trust': [0.00, 0.05, 0.00, 0.10, 0.10, 0.05, 0.05, 0.05, 0.10, 0.05]\n",
    "}\n",
    "survey_10 = pd.DataFrame(survey_data)\n",
    "\n",
    "survey_df = pd.concat([survey_1, survey_2, survey_3, survey_4, survey_5, survey_6, survey_7, survey_8, survey_9, survey_10], ignore_index=True)\n",
    "\n",
    "aggregated_df = survey_df.drop('Participant', axis=1).groupby('song_id').mean().reset_index()\n",
    "\n",
    "# Normalize the distributions so that they sum to 1\n",
    "emotions = ['Anticipation', 'Joy', 'Surprise', 'Anger', 'Disgust', 'Fear', 'Sadness', 'Trust']\n",
    "aggregated_df[emotions] = aggregated_df[emotions].div(aggregated_df[emotions].sum(axis=1), axis=0)\n",
    "\n",
    "survey_df = aggregated_df.sort_values(by='song_id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "id": "M5wGw28vwZiS",
    "outputId": "25491514-eed9-4ecb-c19e-64a6124a5366"
   },
   "outputs": [],
   "source": [
    "# get all emotion data and format\n",
    "list_of_song_ids = survey_df['song_id'].unique().tolist()\n",
    "\n",
    "emotion_df = pd.read_csv('old_emotion_assigned_songs.csv')\n",
    "emotion_df = emotion_df[emotion_df['song_id'].isin(list_of_song_ids)].reset_index()\n",
    "\n",
    "def convert_defaultdict_string(s):\n",
    "    dict_string = s.replace(\"defaultdict(<class 'int'>, \", \"\").strip(\"()\")\n",
    "    return ast.literal_eval(dict_string)\n",
    "\n",
    "emotion_df['emotion_distribution'] = emotion_df['emotion_distribution'].apply(convert_defaultdict_string)\n",
    "\n",
    "emotions = ['anticipation', 'joy', 'surprise', 'anger', 'disgust', 'fear', 'sadness', 'trust']\n",
    "\n",
    "result_df = pd.DataFrame()\n",
    "result_df['song_id'] = emotion_df['song_id']\n",
    "for emotion in emotions:\n",
    "    result_df[emotion.capitalize()] = emotion_df['emotion_distribution'].apply(lambda x: x.get(emotion, 0))\n",
    "\n",
    "emotion_df = result_df.sort_values(by='song_id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "xDuKDYU5wZiT",
    "outputId": "788c0bcf-d816-4daf-a60c-634c8eb130a4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>KL_Divergence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1062758.0</td>\n",
       "      <td>0.946462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4191823.0</td>\n",
       "      <td>0.155166</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4313071.0</td>\n",
       "      <td>0.422628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5955393.0</td>\n",
       "      <td>0.292585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6047243.0</td>\n",
       "      <td>0.492483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6184434.0</td>\n",
       "      <td>0.228473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6636486.0</td>\n",
       "      <td>0.188995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6736901.0</td>\n",
       "      <td>0.379533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6850734.0</td>\n",
       "      <td>0.936562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7402191.0</td>\n",
       "      <td>0.327137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     song_id  KL_Divergence\n",
       "0  1062758.0       0.946462\n",
       "1  4191823.0       0.155166\n",
       "2  4313071.0       0.422628\n",
       "3  5955393.0       0.292585\n",
       "4  6047243.0       0.492483\n",
       "5  6184434.0       0.228473\n",
       "6  6636486.0       0.188995\n",
       "7  6736901.0       0.379533\n",
       "8  6850734.0       0.936562\n",
       "9  7402191.0       0.327137"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate KL Divergence\n",
    "def kl_divergence(p, q):\n",
    "    p = np.array(p)\n",
    "    q = np.array(q)\n",
    "    p = p + 1e-10\n",
    "    q = q + 1e-10\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "results = []\n",
    "for _, row in survey_df.iterrows():\n",
    "    song_id = row['song_id']\n",
    "    true_dist = row[1:].values\n",
    "    predicted_dist = emotion_df[emotion_df['song_id'] == song_id].iloc[0, 1:].values\n",
    "    kl_div = kl_divergence(true_dist, predicted_dist)\n",
    "    results.append({'song_id': song_id, 'KL_Divergence': kl_div})\n",
    "\n",
    "kl_results_df = pd.DataFrame(results)\n",
    "\n",
    "kl_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 218
    },
    "id": "4ipBXhOXwZiV",
    "outputId": "ab6f2de0-2c76-4286-98cb-176a28d9e2ee"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>song_id</th>\n",
       "      <th>self_similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1062758</td>\n",
       "      <td>0.126628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4191823</td>\n",
       "      <td>0.969375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4313071</td>\n",
       "      <td>0.986858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5955393</td>\n",
       "      <td>0.963733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6047243</td>\n",
       "      <td>0.996586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6184434</td>\n",
       "      <td>0.998435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6636486</td>\n",
       "      <td>0.054223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6736901</td>\n",
       "      <td>0.999222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6850734</td>\n",
       "      <td>0.998101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7402191</td>\n",
       "      <td>0.997302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   song_id  self_similarity\n",
       "0  1062758         0.126628\n",
       "1  4191823         0.969375\n",
       "2  4313071         0.986858\n",
       "3  5955393         0.963733\n",
       "4  6047243         0.996586\n",
       "5  6184434         0.998435\n",
       "6  6636486         0.054223\n",
       "7  6736901         0.999222\n",
       "8  6850734         0.998101\n",
       "9  7402191         0.997302"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate Cosine Similarity\n",
    "emotion_features = emotion_df.drop(columns='song_id')\n",
    "survey_features = survey_df.drop(columns='song_id')\n",
    "\n",
    "similarity_matrix = cosine_similarity(emotion_features, survey_features)\n",
    "\n",
    "similarity_df = pd.DataFrame(similarity_matrix, index=emotion_df['song_id'], columns=survey_df['song_id'])\n",
    "\n",
    "self_similarity = similarity_df.values.diagonal()\n",
    "\n",
    "self_similarity_df = pd.DataFrame({\n",
    "    'song_id': emotion_df['song_id'],\n",
    "    'self_similarity': self_similarity\n",
    "})\n",
    "\n",
    "self_similarity_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C39bSzlZwZiV"
   },
   "source": [
    "# Recommendation System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aqA51yKf8Ogr",
    "outputId": "5a462984-ebdf-43ab-be11-220fc78ffb0e"
   },
   "outputs": [],
   "source": [
    "def load_nrc_lexicon():\n",
    "    emotion_lexicon = defaultdict(list)\n",
    "    lexicon_file = \"NRC-Emotion-Lexicon/NRC-Emotion-Lexicon-Wordlevel-v0.92.txt\"\n",
    "    \n",
    "\n",
    "    with open(lexicon_file, 'r') as file:\n",
    "        for line in file:\n",
    "            word, emotion, association = line.strip().split('\\t')\n",
    "            if int(association) == 1:\n",
    "                emotion_lexicon[word].append(emotion)\n",
    "    return emotion_lexicon\n",
    "\n",
    "def preprocess_text(text):\n",
    "    punctuations = '\\'\"\\\\,<>./?@#$%^&*_~/!()-[]{};:'\n",
    "    text = ''.join([char for char in text if char not in punctuations])\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    filtered_tokens = [word for word in tokens if word.isalpha() and word not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "def normalize_distribution(distribution):\n",
    "    total = sum(distribution.values())\n",
    "    return {emotion: value / total for emotion, value in distribution.items()} if total > 0 else distribution\n",
    "\n",
    "def process_user_emotion_input(user_input, emotion_lexicon):\n",
    "    tokens = preprocess_text(user_input)\n",
    "    emotion_count = defaultdict(int)\n",
    "\n",
    "    for word in tokens:\n",
    "        if word in emotion_lexicon:\n",
    "            for emotion in emotion_lexicon[word]:\n",
    "                emotion_count[emotion] += 1\n",
    "\n",
    "    return normalize_distribution(emotion_count)\n",
    "\n",
    "def get_emotion_vector(emotion_dict, emotions_list):\n",
    "    return np.array([emotion_dict.get(emotion, 0) for emotion in emotions_list])\n",
    "\n",
    "def recommend_songs(user_vector, emotion_df, top_n=10):\n",
    "    # Recommend songs based on cosine similarity between the user's emotion vector and song emotion vectors\n",
    "    emotion_matrix = np.array(emotion_df['emotion_vector'].tolist())\n",
    "    similarities = cosine_similarity(user_vector.reshape(1, -1), emotion_matrix).flatten()\n",
    "    emotion_df['cosine_similarity'] = similarities\n",
    "    return emotion_df.nlargest(top_n, 'cosine_similarity')[['title', 'artist', 'cosine_similarity']]\n",
    "\n",
    "# Load the NRC lexicon\n",
    "emotion_lexicon = load_nrc_lexicon()\n",
    "\n",
    "# Load the dataset of songs and convert emotion distributions to dictionaries\n",
    "emotions_df = pd.read_csv('emotion_assigned_songs.csv')\n",
    "emotions_df['emotion_distribution'] = emotions_df['emotion_distribution'].apply(ast.literal_eval)\n",
    "\n",
    "# Define the list of emotions used in the recommendation system\n",
    "emotions = ['anticipation', 'joy', 'surprise', 'anger', 'disgust', 'fear', 'sadness', 'trust']\n",
    "\n",
    "# Convert song emotion distributions to vectors\n",
    "emotions_df['emotion_vector'] = emotions_df['emotion_distribution'].apply(lambda x: get_emotion_vector(x, emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Top 10 Songs Based on Similarity:\n",
      "1. Sweet Scarlet by Cat Stevens - Similarity: 1.0000\n",
      "2. Savannah by LP - Similarity: 1.0000\n",
      "3. Nails Done by Amanda Lepore - Similarity: 1.0000\n",
      "4. Hometown Farewell Kiss by The Triffids - Similarity: 1.0000\n",
      "5. Shes Your Baby by Ween - Similarity: 1.0000\n",
      "6. Good Luck by Kristy Lee Cook - Similarity: 0.9999\n",
      "7. Rainy Day Women 12  35 by Bob Dylan - Similarity: 0.9999\n",
      "8. Dream a Little Dream of Me by Louis Armstrong - Similarity: 0.9998\n",
      "9. When You Came by Rockapella - Similarity: 0.9996\n",
      "10. Black Coffee by Heavy D - Similarity: 0.9996\n"
     ]
    }
   ],
   "source": [
    "# Ask the user for input and process it into an emotion vector\n",
    "user_input = input(\"How are you feeling today? \")\n",
    "user_emotion_distribution = process_user_emotion_input(user_input, emotion_lexicon)\n",
    "user_vector = get_emotion_vector(user_emotion_distribution, emotions)\n",
    "\n",
    "# Get the top 10 recommended songs based on similarity\n",
    "recommended_songs = recommend_songs(user_vector, emotions_df).reset_index()\n",
    "\n",
    "# Print the recommendations\n",
    "print(\"\\n\" + \"-\" * 50 + \"\\n\")\n",
    "print(\"Top 10 Songs Based on Similarity:\")\n",
    "for idx, row in recommended_songs.iterrows():\n",
    "    print(f\"{idx + 1}. {row['title']} by {row['artist']} - Similarity: {row['cosine_similarity']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Uh5IIJ2P_qRf",
    "uFFkQqz2wZiM",
    "0hSZibbJwZiQ",
    "C39bSzlZwZiV"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
